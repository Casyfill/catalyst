# flake8: noqa

from distutils.version import StrictVersion
import logging
from tempfile import TemporaryDirectory

import pytest
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

HAS_REQUIRED_TORCH_VERSION = StrictVersion(torch.__version__) >= StrictVersion("1.8.1")

from catalyst import dl

if HAS_REQUIRED_TORCH_VERSION:
    from catalyst.callbacks import ProfilerCallback

logger = logging.getLogger(__name__)


class DummyDataset(Dataset):
    """Dummy dataset."""

    features_dim: int = 4
    out_dim: int = 2

    def __init__(self, num_records: int):
        self.num_records = num_records

    def __len__(self):
        """
        Returns:
            dataset's length.
        """
        return self.num_records

    def __getitem__(self, idx: int):
        """
        Args:
            idx: index of sample

        Returns:
            dummy features and targets vector
        """
        x = torch.ones(self.features_dim, dtype=torch.float)
        y = torch.ones(self.out_dim, dtype=torch.float)
        return x, y


class DummyModel(nn.Module):
    """Docs."""

    def __init__(self, in_features, out_features):
        """Docs."""
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.layers = nn.Linear(in_features, out_features)

    def forward(self, batch):
        """Docs."""
        return self.layers(batch)


class CustomRunner(dl.IRunner):
    def __init__(self, logdir, device):
        super().__init__()
        self._logdir = logdir
        self._device = device

    def get_engine(self):
        return dl.DeviceEngine(self._device)

    def get_callbacks(self, stage: str):
        return {
            "criterion": dl.CriterionCallback(
                metric_key="loss", input_key="logits", target_key="targets"
            ),
            "optimizer": dl.OptimizerCallback(metric_key="loss"),
            "profiler": ProfilerCallback(
                loader_key="train",
                epoch=1,
                # schedule=torch.profiler.schedule(wait=2, warmup=3, active=6),
                activities=[
                    torch.profiler.ProfilerActivity.CPU,
                    torch.profiler.ProfilerActivity.CUDA,
                ],
                with_stack=True,
                with_flops=True,
            ),
        }

    @property
    def stages(self) -> "Iterable[str]":
        return ["train"]

    def get_stage_len(self, stage: str) -> int:
        return 10

    def get_loaders(self, stage: str) -> "OrderedDict[str, DataLoader]":
        dataset = DummyDataset(6)
        loader = DataLoader(dataset, batch_size=4)
        return {"train": loader, "valid": loader}

    def get_model(self, stage: str):
        return DummyModel(4, 2)

    def get_criterion(self, stage: str):
        return torch.nn.MSELoss()

    def get_optimizer(self, model, stage: str):
        return torch.optim.Adam(model.parameters())

    def get_scheduler(self, optimizer, stage: str):
        return None

    def get_trial(self):
        return None

    def get_loggers(self):
        return {"console": dl.ConsoleLogger(), "csv": dl.CSVLogger(logdir=self._logdir)}

    def handle_batch(self, batch):
        x, y = batch
        logits = self.model(x)

        self.batch = {"features": x, "targets": y, "logits": logits}


def _run_custom_runner(device):
    with TemporaryDirectory() as tmp_dir:
        runner = CustomRunner(tmp_dir, device)
        runner.run()


@pytest.mark.skipif(
    not not HAS_REQUIRED_TORCH_VERSION, reason="Need PyTorch version higher than 1.8.1!"
)
@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA is not available!")
def test_profiler_on_cuda():
    _run_custom_runner("cuda:0")
